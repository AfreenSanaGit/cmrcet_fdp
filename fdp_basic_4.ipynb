pip install llama-index llama-index-embeddings-huggingface llama-index-llms-groq

import logging
import sys
from google.colab import userdata
import os
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
# from llama_index.llms.openai import OpenAI
from llama_index.llms.groq import Groq

logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

documents = SimpleDirectoryReader("data").load_data()

embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")

GROQ_API_KEY = "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"

# os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')

# llm = OpenAI(
#     model="gpt-3.5-turbo",
#     api_key=userdata.get('sk-o60aCUo7BIVicqzB9moWRGmmqi1cipD_DxNfLgYvdRT3BlbkFJBEdMcAOAsfbQoVnuEbEMDBjEm8WMrz16hteNIyjqUA')
# )

llm = Groq(
   model="llama3-70b-8192",
   api_key="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"
)

Settings.embed_model = embed_model
Settings.llm = llm

index = VectorStoreIndex.from_documents(documents)

query_engine = index.as_query_engine()

response = query_engine.query("What is difference between CNN and YOLO?")
print(response)
